{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sushanth.kodali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('FAGE URL Collection - Sheet1.csv')\n",
    "\n",
    "df1 = pd.read_csv('Reviews_RiceCakes_Plain.csv')\n",
    "\n",
    "df2 = pd.read_csv('Reviews_Oatmeal_Plain.csv')\n",
    "\n",
    "\n",
    "from lxml import html  \n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "cols = list(df.columns)[2:3]\n",
    "\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'\n",
    "\n",
    "col_dict = {}\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "for col in cols:\n",
    "    reviews_df = []\n",
    "    for url in df[col]:\n",
    "        \n",
    "#         print(url)\n",
    "        if isinstance(url, float) is False and 'amazon' in url:\n",
    "            headers = {'User-Agent': user_agent}\n",
    "            page = requests.get(url, headers = headers)\n",
    "            parser = html.fromstring(page.content)\n",
    "            xpath_reviews = '//div[@data-hook=\"review\"]'\n",
    "            reviews = parser.xpath(xpath_reviews)\n",
    "            xpath_rating  = './/i[@data-hook=\"review-star-rating\"]//text()' \n",
    "            xpath_title   = './/a[@data-hook=\"review-title\"]//text()'\n",
    "            xpath_author  = './/a[@data-hook=\"review-author\"]//text()'\n",
    "            xpath_date    = './/span[@data-hook=\"review-date\"]//text()'\n",
    "            xpath_body    = './/span[@data-hook=\"review-body\"]//text()'\n",
    "            xpath_helpful = './/span[@data-hook=\"helpful-vote-statement\"]//text()'\n",
    "\n",
    "            for review in reviews:\n",
    "                rating  = review.xpath(xpath_rating)\n",
    "                title   = review.xpath(xpath_title)\n",
    "                author  = review.xpath(xpath_author)\n",
    "                date    = review.xpath(xpath_date)\n",
    "                body    = review.xpath(xpath_body)\n",
    "                helpful = review.xpath(xpath_helpful)\n",
    "                \n",
    "                \n",
    "#                 print(body)\n",
    "                \n",
    "                word_tokens = word_tokenize(body[0])\n",
    "\n",
    "                filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "                filtered_sentence = []\n",
    "\n",
    "                for w in word_tokens:\n",
    "                    if w not in stop_words:\n",
    "                        filtered_sentence.append(lmtzr.lemmatize(w).lower())\n",
    "\n",
    "                review_dict = {'rating': rating,\n",
    "                               'title': title,\n",
    "                               'author': author,             \n",
    "                               'date': date,\n",
    "                               'body': \" \".join(filtered_sentence),\n",
    "                               'helpful': helpful}\n",
    "#                 print(review_dict)\n",
    "                reviews_df.append(review_dict)\n",
    "    print(len(reviews_df))    \n",
    "    col_dict[col] = pd.DataFrame(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "cols1 = list(df1.columns)\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'\n",
    "\n",
    "col_dict1 = {}\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "for col in cols1:\n",
    "    reviews_df = []\n",
    "    for url in df1[col]:\n",
    "        \n",
    "#         print(url)\n",
    "        if isinstance(url, float) is False and 'amazon' in url:\n",
    "            headers = {'User-Agent': user_agent}\n",
    "            page = requests.get(url, headers = headers)\n",
    "            parser = html.fromstring(page.content)\n",
    "            xpath_reviews = '//div[@data-hook=\"review\"]'\n",
    "            reviews = parser.xpath(xpath_reviews)\n",
    "            xpath_rating  = './/i[@data-hook=\"review-star-rating\"]//text()' \n",
    "            xpath_title   = './/a[@data-hook=\"review-title\"]//text()'\n",
    "            xpath_author  = './/a[@data-hook=\"review-author\"]//text()'\n",
    "            xpath_date    = './/span[@data-hook=\"review-date\"]//text()'\n",
    "            xpath_body    = './/span[@data-hook=\"review-body\"]//text()'\n",
    "            xpath_helpful = './/span[@data-hook=\"helpful-vote-statement\"]//text()'\n",
    "\n",
    "            for review in reviews:\n",
    "                rating  = review.xpath(xpath_rating)\n",
    "                title   = review.xpath(xpath_title)\n",
    "                author  = review.xpath(xpath_author)\n",
    "                date    = review.xpath(xpath_date)\n",
    "                body    = review.xpath(xpath_body)\n",
    "                helpful = review.xpath(xpath_helpful)\n",
    "                \n",
    "                \n",
    "#                 print(body)\n",
    "                \n",
    "                word_tokens = word_tokenize(body[0])\n",
    "\n",
    "                filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "                filtered_sentence = []\n",
    "\n",
    "                for w in word_tokens:\n",
    "                    if w not in stop_words:\n",
    "                        filtered_sentence.append(lmtzr.lemmatize(w).lower())\n",
    "\n",
    "                review_dict = {'rating': rating,\n",
    "                               'title': title,\n",
    "                               'author': author,             \n",
    "                               'date': date,\n",
    "                               'body': \" \".join(filtered_sentence),\n",
    "                               'helpful': helpful}\n",
    "#                 print(review_dict)\n",
    "                reviews_df.append(review_dict)\n",
    "    print(len(reviews_df))    \n",
    "    col_dict1[col] = pd.DataFrame(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "cols2 = list(df2.columns)\n",
    "\n",
    "\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'\n",
    "\n",
    "col_dict2 = {}\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "for col in cols2:\n",
    "    reviews_df = []\n",
    "    for url in df2[col]:\n",
    "        \n",
    "#         print(url)\n",
    "        if isinstance(url, float) is False and 'amazon' in url:\n",
    "            headers = {'User-Agent': user_agent}\n",
    "            page = requests.get(url, headers = headers)\n",
    "            parser = html.fromstring(page.content)\n",
    "            xpath_reviews = '//div[@data-hook=\"review\"]'\n",
    "            reviews = parser.xpath(xpath_reviews)\n",
    "            xpath_rating  = './/i[@data-hook=\"review-star-rating\"]//text()' \n",
    "            xpath_title   = './/a[@data-hook=\"review-title\"]//text()'\n",
    "            xpath_author  = './/a[@data-hook=\"review-author\"]//text()'\n",
    "            xpath_date    = './/span[@data-hook=\"review-date\"]//text()'\n",
    "            xpath_body    = './/span[@data-hook=\"review-body\"]//text()'\n",
    "            xpath_helpful = './/span[@data-hook=\"helpful-vote-statement\"]//text()'\n",
    "\n",
    "            for review in reviews:\n",
    "                rating  = review.xpath(xpath_rating)\n",
    "                title   = review.xpath(xpath_title)\n",
    "                author  = review.xpath(xpath_author)\n",
    "                date    = review.xpath(xpath_date)\n",
    "                body    = review.xpath(xpath_body)\n",
    "                helpful = review.xpath(xpath_helpful)\n",
    "                \n",
    "                \n",
    "#                 print(body)\n",
    "                \n",
    "                word_tokens = word_tokenize(body[0])\n",
    "\n",
    "                filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "                filtered_sentence = []\n",
    "\n",
    "                for w in word_tokens:\n",
    "                    if w not in stop_words:\n",
    "                        filtered_sentence.append(lmtzr.lemmatize(w).lower())\n",
    "\n",
    "                review_dict = {'rating': rating,\n",
    "                               'title': title,\n",
    "                               'author': author,             \n",
    "                               'date': date,\n",
    "                               'body': \" \".join(filtered_sentence),\n",
    "                               'helpful': helpful}\n",
    "#                 print(review_dict)\n",
    "                reviews_df.append(review_dict)\n",
    "    print(len(reviews_df))    \n",
    "    col_dict2[col] = pd.DataFrame(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentences1 = []\n",
    "sentences2 = []\n",
    "\n",
    "string = []\n",
    "\n",
    "string1 = []\n",
    "\n",
    "string2 = []\n",
    "\n",
    "for key in col_dict:\n",
    "    sentences = list([wd.split() for wd in list(col_dict[key]['body'])])\n",
    "    \n",
    "for key in col_dict1:\n",
    "    sentences1 = list([wd.split() for wd in list(col_dict1[key]['body'])])\n",
    "\n",
    "    \n",
    "# print(col_dict2)\n",
    "for key in col_dict2:\n",
    "    sentences2 = list([wd.split() for wd in list(col_dict2[key]['body'])])\n",
    "    \n",
    "for sent in sentences:\n",
    "#     print(sent)\n",
    "    string.append(' '.join(sent))\n",
    "    \n",
    "for sent in sentences1:\n",
    "    string1.append(' '.join(sent))\n",
    "    \n",
    "for sent in sentences2:\n",
    "#     print(sent)\n",
    "    string2.append(' '.join(sent))\n",
    "    \n",
    "# print(sentences2)\n",
    "    \n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sushanth.kodali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sushanth.kodali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = ['’','also','.and','a+','It','it','like','I','i',',','.','n\\'t','...','!','the','The','\\'','\\'s','?','\\'ve','&','\\'\\'','(',')','^^','_','-','``','\\'m']\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "en_stop = list(en_stop)+stop_words\n",
    "en_stop = set(en_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 2 and not any(p in token for p in punctuation)]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "text_data1 = []\n",
    "text_data2 = []\n",
    "for line in string:\n",
    "    tokens = prepare_text_for_lda(line)\n",
    "#     print(tokens)\n",
    "    text_data.append(tokens)\n",
    "    \n",
    "for line in string1:\n",
    "    tokens = prepare_text_for_lda(line)\n",
    "#     print(tokens)\n",
    "    text_data1.append(tokens)\n",
    "\n",
    "for line in string2:\n",
    "    tokens = prepare_text_for_lda(line)\n",
    "#     print(tokens)\n",
    "    text_data2.append(tokens)\n",
    "\n",
    "# print(text_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "dictionary1 = corpora.Dictionary(text_data1)\n",
    "dictionary2 = corpora.Dictionary(text_data2)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "corpus1 = [dictionary1.doc2bow(text) for text in text_data1]\n",
    "corpus2 = [dictionary2.doc2bow(text) for text in text_data2]\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')\n",
    "dictionary1.save('dictionary1.gensim')\n",
    "dictionary2.save('dictionary2.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['greek', 'yogurt', 'love', 'style', 'fat', 'free', 'rich', 'creamy', 'ever', 'thank']\n",
      "['yogurt', 'protein', 'sugar', 'love', 'greek', 'fruit', 'low', 'cream', 'mix', 'thick']\n",
      "['consistence', 'real', 'yogurt', 'bad', 'disappoint', 'old', 'joghurt', 'plane', 'ingredient', 'dip']\n",
      "['yogurt', 'good', 'greek', 'cow', 'milk', 'brown', 'bad', 'whole', 'protein', 'real']\n",
      "['yogurt', 'best', 'texture', 'add', 'greek', 'taste', 'also', 'great', 'smoothy', 'fruit']\n",
      "['mix', 'thick', 'smell', 'strawberry', 'walnut', 'yummy', 'banana', 'fresh', 'healthy', 'creamy']\n",
      "['yogurt', 'greek', 'add', 'whip', 'besides', 'plain', 'best', 'organic', 'exactly', 'could']\n",
      "['smoothy', 'loss', 'delicious', 'creamy', 'get', 'plus', 'weight', 'great', 'job', 'diet']\n",
      "['keep', 'full', 'stuff', 'healthy', 'fat', 'need', 'market', 'flavor', 'instead', 'greece']\n",
      "['like', 'yogurt', 'fantastic', 'greek', 'purchase', 'price', 'best', 'taste', 'fresh', 'flavor']\n",
      "['sugar', 'eat', 'love', 'low', 'intake', 'fruit', 'year', 'granola', 'daily', 'creamy']\n",
      "['yogurt', 'love', 'try', 'husband', 'rest', 'better', 'taste', 'far', 'say', 'greek']\n",
      "['yogurt', 'greek', 'brand', 'taste', 'plain', 'thick', 'perfect', 'buy', 'good', 'creamy']\n",
      "['good', 'enjoy', 'value', 'whole', 'price', 'family', 'plain', 'slice', 'desert', 'tarte']\n",
      "['yogurt', 'greek', 'great', 'worth', 'tart', 'fat', 'bland', 'compare', 'dressing', 'everything']\n",
      "['yogurt', 'greek', 'love', 'best', 'nonfat', 'thick', 'smooth', 'though', 'dish', 'sweet']\n",
      "['delicious', 'creamy', 'add', 'others', 'mostly', 'nonfat', 'brand', 'also', 'consistency', 'greek']\n",
      "['greek', 'quality', 'yogurt', 'tub', 'great', 'many', 'eating', 'base', 'god', 'cost']\n",
      "['household', 'hand', 'stuff', 'keep', 'texture', 'instead', 'dip', 'chip', 'staple', 'daily']\n",
      "['product', 'yogurt', 'try', 'one', 'organic', 'prefer', 'make', 'like', 'plain', 'add']\n",
      "\n",
      "\n",
      "              0        1          2           3           4        5        6  \\\n",
      "0         greek   yogurt       love       style         fat     free     rich   \n",
      "1        yogurt  protein      sugar        love       greek    fruit      low   \n",
      "2   consistence     real     yogurt         bad  disappoint      old  joghurt   \n",
      "3        yogurt     good      greek         cow        milk    brown      bad   \n",
      "4        yogurt     best    texture         add       greek    taste     also   \n",
      "5           mix    thick      smell  strawberry      walnut    yummy   banana   \n",
      "6        yogurt    greek        add        whip     besides    plain     best   \n",
      "7       smoothy     loss  delicious      creamy         get     plus   weight   \n",
      "8          keep     full      stuff     healthy         fat     need   market   \n",
      "9          like   yogurt  fantastic       greek    purchase    price     best   \n",
      "10        sugar      eat       love         low      intake    fruit     year   \n",
      "11       yogurt     love        try     husband        rest   better    taste   \n",
      "12       yogurt    greek      brand       taste       plain    thick  perfect   \n",
      "13         good    enjoy      value       whole       price   family    plain   \n",
      "14       yogurt    greek      great       worth        tart      fat    bland   \n",
      "15       yogurt    greek       love        best      nonfat    thick   smooth   \n",
      "16    delicious   creamy        add      others      mostly   nonfat    brand   \n",
      "17        greek  quality     yogurt         tub       great     many   eating   \n",
      "18    household     hand      stuff        keep     texture  instead      dip   \n",
      "19      product   yogurt        try         one     organic   prefer     make   \n",
      "\n",
      "          7            8           9  \n",
      "0    creamy         ever       thank  \n",
      "1     cream          mix       thick  \n",
      "2     plane   ingredient         dip  \n",
      "3     whole      protein        real  \n",
      "4     great      smoothy       fruit  \n",
      "5     fresh      healthy      creamy  \n",
      "6   organic      exactly       could  \n",
      "7     great          job        diet  \n",
      "8    flavor      instead      greece  \n",
      "9     taste        fresh      flavor  \n",
      "10  granola        daily      creamy  \n",
      "11      far          say       greek  \n",
      "12      buy         good      creamy  \n",
      "13    slice       desert       tarte  \n",
      "14  compare     dressing  everything  \n",
      "15   though         dish       sweet  \n",
      "16     also  consistency       greek  \n",
      "17     base          god        cost  \n",
      "18     chip       staple       daily  \n",
      "19     like        plain         add  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "NUM_TOPICS = 25\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "ldamodel1 = gensim.models.ldamodel.LdaModel(corpus1, num_topics = NUM_TOPICS, id2word=dictionary1, passes=15)\n",
    "ldamodel1.save('model51.gensim')\n",
    "ldamodel2 = gensim.models.ldamodel.LdaModel(corpus2, num_topics = NUM_TOPICS, id2word=dictionary2, passes=15)\n",
    "ldamodel2.save('model52.gensim')\n",
    "\n",
    "# top_words_per_topic = []\n",
    "# for t in range(ldamodel.num_topics):\n",
    "#     top_words_per_topic.extend([(t, ) + x for x in ldamodel.show_topic(t, topn = 10)])\n",
    "\n",
    "# pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"plain_greek.csv\")\n",
    "\n",
    "# top_words_per_topic1 = []\n",
    "# for t in range(ldamodel1.num_topics):\n",
    "#     top_words_per_topic.extend([(t, ) + x for x in ldamodel1.show_topic(t, topn = 10)])\n",
    "\n",
    "# pd.DataFrame(top_words_per_topic1, columns=['Topic', 'Word', 'P']).to_csv(\"rice_cake.csv\")\n",
    "\n",
    "# top_words_per_topic2 = []\n",
    "# for t in range(ldamodel2.num_topics):\n",
    "#     top_words_per_topic.extend([(t, ) + x for x in ldamodel2.show_topic(t, topn = 10)])\n",
    "\n",
    "# pd.DataFrame(top_words_per_topic2, columns=['Topic', 'Word', 'P']).to_csv(\"oatmeal.csv\")\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "dl = []\n",
    "dl1 = []\n",
    "dl2 = []\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    td = {}\n",
    "    if topic not in en_stop:\n",
    "        print(regex.sub(' ', topic[1]).split())\n",
    "        dl.append(regex.sub(' ', topic[1]).split())\n",
    "print()\n",
    "topics1 = ldamodel1.print_topics(num_words=10)\n",
    "for topic in topics1:\n",
    "    if topic not in en_stop:\n",
    "        dl1.append(regex.sub(' ', topic[1]).split())\n",
    "print()    \n",
    "topics2 = ldamodel2.print_topics(num_words=10)\n",
    "for topic in topics2:\n",
    "    if topic not in en_stop:\n",
    "        dl2.append(regex.sub(' ', topic[1]).split())\n",
    "        \n",
    "tdf = pd.DataFrame(dl)\n",
    "tdf1 = pd.DataFrame(dl1)\n",
    "tdf2 = pd.DataFrame(dl2)\n",
    "\n",
    "print(tdf)\n",
    "\n",
    "# tdf.columns = ['Topic1','Topic2','Topic2','Topic3','Topic3','Topic4','Topic5','Topic6','Topic7','Topic8','Topic9','Topic10']\n",
    "# tdf1.columns = ['Topic1','Topic2','Topic2','Topic3','Topic3','Topic4','Topic5','Topic6','Topic7','Topic8','Topic9','Topic10']\n",
    "# tdf2.columns = ['Topic1','Topic2','Topic2','Topic3','Topic3','Topic4','Topic5','Topic6','Topic7','Topic8','Topic9','Topic10']\n",
    "\n",
    "\n",
    "tdf.to_csv('plain_greek.csv')\n",
    "tdf1.to_csv('rice_cake.csv')\n",
    "tdf2.to_csv('oatmeal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)\n",
    "\n",
    "dictionary1 = gensim.corpora.Dictionary.load('dictionary1.gensim')\n",
    "lda1 = gensim.models.ldamodel.LdaModel.load('model51.gensim')\n",
    "lda_display1 = pyLDAvis.gensim.prepare(lda1, corpus, dictionary1, sort_topics=False)\n",
    "pyLDAvis.display(lda_display1)\n",
    "\n",
    "dictionary2 = gensim.corpora.Dictionary.load('dictionary2.gensim')\n",
    "lda2 = gensim.models.ldamodel.LdaModel.load('model52.gensim')\n",
    "lda_display2 = pyLDAvis.gensim.prepare(lda2, corpus, dictionary2, sort_topics=False)\n",
    "pyLDAvis.display(lda_display2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
