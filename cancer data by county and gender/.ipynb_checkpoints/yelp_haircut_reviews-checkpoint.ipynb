{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing all libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Github; how to replace and expand contactions\n",
    "import re\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\cdiaz\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "#Headers will make it look like you are using a web browser\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}\n",
    "#We will use the iteration to retrieve and scrape the web pages, reviews, and ratings from each page on Yelp\n",
    "reviews = []\n",
    "ratings = [] \n",
    "\n",
    "\n",
    "    for i in range (0,500,20):\n",
    "        url = 'https://www.yelp.com/biz/m%C3%A9m%C3%A9-mediterranean-new-york-4?start={}'.format(i)\n",
    "        response = requests.get(url, headers=headers, verify=False).text\n",
    "        soup = BeautifulSoup(response, \"lxml\")\n",
    "    #Looping through 'div' 'review-content' will help find all the review containers we need in each page that have rating and review\n",
    "        for s in soup.find_all(\"div\", attrs={'class': 'review-content'}):\n",
    "            re = s.find('p', attrs={'lang': 'en'})\n",
    "    #Makes all the letters lower in reviews\n",
    "            review = re.text.lower()\n",
    "    #expandContractions will put the dictionary made earlier to replace the contractions in the reviews\n",
    "    #Make sure to to run the cList dict cell or else there will be an error\n",
    "            expandContractions(review)\n",
    "    #Cleaning the lemmas or words in reviews now will make it easier when we start predictive modeling\n",
    "            words = word_tokenize(review)\n",
    "            words = word_tokenize(review.replace('\\n',' '))\n",
    "            clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "            characters_to_remove = [\"''\",'``','...']\n",
    "            clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "            english_stops = set(stopwords.words('english'))\n",
    "            clean_words = [word for word in clean_words if word not in english_stops]\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "            reviews.append(lemma_list)\n",
    "    #Here we are using a simple control flow to recode the ratings for our model. If rating is greater than 3 positive, else negative   \n",
    "            rating = s.find_all('img', attrs={'class': 'offscreen'})\n",
    "    #the rating is actually an image, so we need to convert it into a string and then to an integer\n",
    "            rate = str(rating)\n",
    "            int_rating = int(rate[11:12])\n",
    "\n",
    "            if int_rating == 1 or int_rating == 2 or int_rating == 3:\n",
    "                rating = 'neg'\n",
    "                ratings.append('neg')\n",
    "            elif int_rating == 4 or int_rating == 5:\n",
    "                rating = 'pos'\n",
    "                ratings.append('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Making sure the number of reviews and ratings match before we append them for our featureset\n",
    "print(len(reviews))\n",
    "print(len(ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #1: Let's first model **UNIGRAMS** & Naives Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    rude = True              neg : pos    =     24.6 : 1.0\n",
      "           unfortunately = True              neg : pos    =     15.2 : 1.0\n",
      "                    okay = True              neg : pos    =     14.8 : 1.0\n",
      "                terrible = True              neg : pos    =     12.9 : 1.0\n",
      "                     pay = True              neg : pos    =     12.9 : 1.0\n",
      "                  medium = True              neg : pos    =     10.5 : 1.0\n",
      "                   empty = True              neg : pos    =     10.5 : 1.0\n",
      "                  either = True              neg : pos    =     10.5 : 1.0\n",
      "                  seeing = True              neg : pos    =      8.2 : 1.0\n",
      "                 leaving = True              neg : pos    =      8.2 : 1.0\n",
      "0.7333333333333333\n",
      "pos precision: 0.9325842696629213\n",
      "pos recall: 0.7094017094017094\n",
      "pos F-measure: 0.8058252427184467\n",
      "neg precision: 0.4426229508196721\n",
      "neg recall: 0.8181818181818182\n",
      "neg F-measure: 0.5744680851063829\n"
     ]
    }
   ],
   "source": [
    "rl = zip(reviews,ratings)\n",
    "\n",
    "#define a bag_of_words function to return word, True.\n",
    "\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# Define another function that will return words that are in words, but not in badwords\n",
    "\n",
    "def bag_of_words_not_in_set(words, badwords):\n",
    "    return bag_of_words(set(words) - set(badwords))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#define a bag_of_non_stopwords function to return word, True.\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile='english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "#Creating our unigram featureset dictionary for modeling\n",
    "\n",
    "uni_featureset = []\n",
    "\n",
    "for k, v in rl:\n",
    "    bag_of_words(k)\n",
    "    uni_featureset.append((bag_of_words(k),v))\n",
    "\n",
    "import random\n",
    "random.shuffle(uni_featureset)\n",
    "\n",
    "#splits the data around 70% of 500 *350 reviews* for both testing and training\n",
    "\n",
    "train_set, test_set = uni_featureset[0:350], uni_featureset[350:]\n",
    "\n",
    "#Now we will calculate accuracy, precision, recall, and f-measure using Naives Bayes classifier\n",
    "#This will also show the top 10 most informative features of our featureset\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nb_classifier.show_most_informative_features(10)\n",
    "\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(nb_classifier, test_set))\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "    \n",
    "for i, (uni_featureset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(uni_featureset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #2: **UNIGRAMS** & Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n",
      "pos precision: 0.8125\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.896551724137931\n",
      "neg precision: 0.4426229508196721\n",
      "neg recall: 0.8181818181818182\n",
      "neg F-measure: 0.5744680851063829\n"
     ]
    }
   ],
   "source": [
    "#Making a decision tree model to compare which is the better performing model\n",
    "import collections\n",
    "from nltk import metrics\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(dt_classifier, test_set))\n",
    "    \n",
    "for i, (uni_featureset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = dt_classifier.classify(uni_featureset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #3: **UNIGRAMS** & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.78\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8764044943820225\n",
      "neg precision: 0.4426229508196721\n",
      "neg recall: 0.8181818181818182\n",
      "neg F-measure: 0.5744680851063829\n"
     ]
    }
   ],
   "source": [
    "#Create Logistic Regression model to compare which is the better performing model\n",
    "from nltk.classify import MaxentClassifier\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    " \n",
    "for i, (uni_featureset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = logit_classifier.classify(uni_featureset)\n",
    "    testsets[observed].add(i)\n",
    "  \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #4: **UNIGRAMS** & SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.78\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8764044943820225\n",
      "neg precision: 0.4426229508196721\n",
      "neg recall: 0.8181818181818182\n",
      "neg F-measure: 0.5744680851063829\n"
     ]
    }
   ],
   "source": [
    "# #Create an SVM to compare which is the better performing model\n",
    "\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    "\n",
    "for i, (uni_featureset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = SVM_classifier.classify(uni_featureset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #5 In order to get more context, we should start modeling **BIGRAMS** & Naive Bayes with the same dataset and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "  ('nothing', 'special') = True              neg : pos    =      9.4 : 1.0\n",
      "        ('would', \"n't\") = True              neg : pos    =      7.3 : 1.0\n",
      "        ('good', 'went') = True              neg : pos    =      6.7 : 1.0\n",
      "          ('bus', 'boy') = True              neg : pos    =      6.7 : 1.0\n",
      "           ('told', 'u') = True              neg : pos    =      6.7 : 1.0\n",
      "      ('second', 'time') = True              neg : pos    =      6.7 : 1.0\n",
      "           ('3', 'star') = True              neg : pos    =      6.7 : 1.0\n",
      " ('linguine', 'seafood') = True              neg : pos    =      6.7 : 1.0\n",
      "        ('next', 'door') = True              neg : pos    =      6.7 : 1.0\n",
      "        ('food', 'even') = True              neg : pos    =      6.7 : 1.0\n",
      "0.5933333333333334\n",
      "pos precision: 0.8636363636363636\n",
      "pos recall: 0.5229357798165137\n",
      "pos F-measure: 0.6514285714285714\n",
      "neg precision: 0.38095238095238093\n",
      "neg recall: 0.7804878048780488\n",
      "neg F-measure: 0.512\n"
     ]
    }
   ],
   "source": [
    "rl = zip(reviews,ratings)\n",
    "\n",
    "#define a bag_of_words function to return word, True. Only I am renaming it to the hotel's name for simplicity\n",
    "\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "def bag_of_words_not_in_set(words, badwords):\n",
    "    return bag_of_words(set(words) - set(badwords))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile='english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "# Import Bigram finder\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "# Import Bigram metrics - we will use these to identify the top 200 bigrams\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=100):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return bag_of_words(bigrams)\n",
    "    \n",
    "    bigrams = bag_of_bigrams_words(words)\n",
    "\n",
    "#Creating our bigram featureset dictionary for modeling\n",
    "\n",
    "featureset = []\n",
    "\n",
    "for t, v in rl:\n",
    "    bag_of_bigrams_words(t)\n",
    "    featureset.append((bag_of_bigrams_words(t),v))\n",
    "\n",
    "import random\n",
    "random.shuffle(featureset)\n",
    "\n",
    "#splits the data around 70% of 500 *350 reviews* for both testing and training\n",
    "\n",
    "train_set, test_set = featureset[0:350], featureset[350:]\n",
    "\n",
    "#Now we will calculate accuracy, precision, recall, and f-measure using Naives Bayes classifier\n",
    "#This will also show the top 10 most informative features of our featureset\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nb_classifier.show_most_informative_features(10)\n",
    "\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(nb_classifier, test_set))\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "    \n",
    "for i, (featureset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(featureset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #6: **BIGRAMS** & Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7266666666666667\n",
      "pos precision: 0.7297297297297297\n",
      "pos recall: 0.9908256880733946\n",
      "pos F-measure: 0.8404669260700388\n",
      "neg precision: 0.38095238095238093\n",
      "neg recall: 0.7804878048780488\n",
      "neg F-measure: 0.512\n"
     ]
    }
   ],
   "source": [
    "#Making a decision tree model to compare which is the better performing model\n",
    "import collections\n",
    "from nltk import metrics\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(dt_classifier, test_set))\n",
    "    \n",
    "for i, (featureset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = dt_classifier.classify(featureset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #7: **BIGRAMS** & Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.7266666666666667\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8416988416988417\n",
      "neg precision: 0.38095238095238093\n",
      "neg recall: 0.7804878048780488\n",
      "neg F-measure: 0.512\n"
     ]
    }
   ],
   "source": [
    "#Create Logistic Regression model to compare\n",
    "from nltk.classify import MaxentClassifier\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    " \n",
    "for i, (featureset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = logit_classifier.classify(featureset)\n",
    "    testsets[observed].add(i)\n",
    "  \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #8: **BIGRAMS** & SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.7266666666666667\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8416988416988417\n",
      "neg precision: 0.38095238095238093\n",
      "neg recall: 0.7804878048780488\n",
      "neg F-measure: 0.512\n"
     ]
    }
   ],
   "source": [
    "#Create an SVM Model to compare\n",
    "\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    "\n",
    "for i, (featureset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = SVM_classifier.classify(featureset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #9: To get even more context, we should start modeling **TRIGRAMS** & Naive Bayes with the same dataset and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "('nice', 'wait', 'staff') = True              neg : pos    =      5.6 : 1.0\n",
      "('lobster', 'beet', 'risotto') = True              neg : pos    =      3.3 : 1.0\n",
      "('egg', 'spicy', 'tomato') = True              neg : pos    =      3.3 : 1.0\n",
      "('service', 'pretty', 'good') = True              neg : pos    =      3.3 : 1.0\n",
      "('little', 'bit', 'everything') = True              neg : pos    =      3.3 : 1.0\n",
      "('food', 'excellent', 'service') = True              neg : pos    =      3.3 : 1.0\n",
      " ('would', 'go', 'back') = True              neg : pos    =      3.3 : 1.0\n",
      "('absolutely', 'love', 'place') = True              neg : pos    =      3.3 : 1.0\n",
      "('matbucha', 'carrot', 'roasted') = True              neg : pos    =      3.3 : 1.0\n",
      "('ordered', '4', 'dish') = True              neg : pos    =      3.3 : 1.0\n",
      "0.7333333333333333\n",
      "pos precision: 0.7985074626865671\n",
      "pos recall: 0.8916666666666667\n",
      "pos F-measure: 0.84251968503937\n",
      "neg precision: 0.1875\n",
      "neg recall: 0.1\n",
      "neg F-measure: 0.13043478260869565\n"
     ]
    }
   ],
   "source": [
    "rl = zip(reviews,ratings)\n",
    "\n",
    "#define a bag_of_words function to return word, True. Only I am renaming it to the hotel's name for simplicity\n",
    "\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile='english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "# Import Bigram finder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "\n",
    "# Import Bigram metrics - we will use these to identify the top 200 bigrams\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "def bag_of_trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq, n=100):\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(words)\n",
    "    trigrams = trigram_finder.nbest(score_fn, n)\n",
    "    return bag_of_words(trigrams)\n",
    "    \n",
    "    trigrams = bag_of_trigrams_words(words)\n",
    "\n",
    "#Creating our featureset dictionary for modeling\n",
    "\n",
    "featureset2 = []\n",
    "\n",
    "for k, v in rl:\n",
    "    bag_of_trigrams_words(k)\n",
    "    featureset2.append((bag_of_trigrams_words(k),v))\n",
    "\n",
    "import random\n",
    "random.shuffle(featureset2)\n",
    "\n",
    "#splits the data around 70% of 500 *350 reviews* for both testing and training\n",
    "\n",
    "train_set, test_set = featureset2[0:350], featureset2[350:]\n",
    "\n",
    "#Now we will calculate accuracy, precision, recall, and f-measure using Naives Bayes classifier\n",
    "#This will also show the top 10 most informative features of our featureset\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nb_classifier.show_most_informative_features(10)\n",
    "\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(nb_classifier, test_set))\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "    \n",
    "for i, (featureset2, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(featureset2)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #10: **TRIGRAMS** & Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "pos precision: 0.8\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8888888888888888\n",
      "neg precision: 0.1875\n",
      "neg recall: 0.1\n",
      "neg F-measure: 0.13043478260869565\n"
     ]
    }
   ],
   "source": [
    "#Making a decision tree model to compare which is the better performing model\n",
    "import collections\n",
    "from nltk import metrics\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(dt_classifier, test_set))\n",
    "    \n",
    "for i, (featureset2, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = dt_classifier.classify(featureset2)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #11: **TRIGRAMS** & Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.8\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8888888888888888\n",
      "neg precision: 0.17647058823529413\n",
      "neg recall: 0.1\n",
      "neg F-measure: 0.12765957446808512\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    " \n",
    "for i, (featureset2, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = logit_classifier.classify(featureset2)\n",
    "    testsets[observed].add(i)\n",
    "  \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #12: **TRIGRAMS** & SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.8\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8888888888888888\n",
      "neg precision: 0.17647058823529413\n",
      "neg recall: 0.1\n",
      "neg F-measure: 0.12765957446808512\n"
     ]
    }
   ],
   "source": [
    "# SVM model\n",
    "\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    "\n",
    "for i, (featureset2, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = SVM_classifier.classify(featureset2)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I was unable to properly model the ngram combination, I was able to look at **unigrams + bigrams** and **unigrams + trigrams** seperately.\n",
    "I still however want to learn how to do it properly after this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #13: **UNIGRAMS + BIGRAMS** & Naives Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    rude = True              neg : pos    =     27.0 : 1.0\n",
      "                    okay = True              neg : pos    =     22.7 : 1.0\n",
      "                 someone = True              neg : pos    =     16.2 : 1.0\n",
      "                   empty = True              neg : pos    =     14.0 : 1.0\n",
      "                  rather = True              neg : pos    =     14.0 : 1.0\n",
      "  ('nothing', 'special') = True              neg : pos    =     11.9 : 1.0\n",
      "           ('told', 'u') = True              neg : pos    =     11.9 : 1.0\n",
      "              overcooked = True              neg : pos    =     11.9 : 1.0\n",
      "                    bill = True              neg : pos    =     11.9 : 1.0\n",
      "                 clearly = True              neg : pos    =      9.7 : 1.0\n",
      "0.58\n",
      "pos precision: 0.9836065573770492\n",
      "pos recall: 0.4918032786885246\n",
      "pos F-measure: 0.6557377049180328\n",
      "neg precision: 0.30337078651685395\n",
      "neg recall: 0.9642857142857143\n",
      "neg F-measure: 0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "rl = zip(reviews,ratings)\n",
    "\n",
    "#define a bag_of_words function to return word, True.\n",
    "\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# Define another function that will return words that are in words, but not in badwords\n",
    "\n",
    "def bag_of_words_not_in_set(words, badwords):\n",
    "    return bag_of_words(set(words) - set(badwords))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#define a bag_of_non_stopwords function to return word, True.\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile='english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "# Import Bigram metrics - we will use these to identify the top 200 bigrams\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=100):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return bag_of_words(bigrams + words)\n",
    "    \n",
    "    bigrams = bag_of_bigrams_words(words)\n",
    "\n",
    "#Creating our featureset dictionary for modeling\n",
    "\n",
    "unibi_fset = []\n",
    "\n",
    "for k, v in rl:\n",
    "    bag_of_bigrams_words(k)\n",
    "    unibi_fset.append((bag_of_bigrams_words(k),v))\n",
    "\n",
    "import random\n",
    "random.shuffle(unibi_fset)\n",
    "\n",
    "#splits the data around 70% of 500 *350 reviews* for both testing and training\n",
    "\n",
    "train_set, test_set = unibi_fset[0:350], unibi_fset[350:]\n",
    "\n",
    "#Now we will calculate accuracy, precision, recall, and f-measure using Naives Bayes classifier\n",
    "#This will also show the top 10 most informative features of our featureset\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nb_classifier.show_most_informative_features(10)\n",
    "\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(nb_classifier, test_set))\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "    \n",
    "for i, (unibi_fset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(unibi_fset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #14: **UNIGRAMS + BIGRAMS** & Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7933333333333333\n",
      "pos precision: 0.7891156462585034\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8821292775665399\n",
      "neg precision: 0.36585365853658536\n",
      "neg recall: 0.8823529411764706\n",
      "neg F-measure: 0.5172413793103449\n"
     ]
    }
   ],
   "source": [
    "#Making a decision tree model to compare which is the better performing model\n",
    "import collections\n",
    "from nltk import metrics\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(dt_classifier, test_set))\n",
    "    \n",
    "for i, (unibi_fset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = dt_classifier.classify(unibi_fset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #15: **UNIGRAMS + BIGRAMS** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.7733333333333333\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8721804511278195\n",
      "neg precision: 0.36585365853658536\n",
      "neg recall: 0.8823529411764706\n",
      "neg F-measure: 0.5172413793103449\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    " \n",
    "for i, (unibi_fset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = logit_classifier.classify(unibi_fset)\n",
    "    testsets[observed].add(i)\n",
    "  \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try **UNIGRAMS + TRIGRAMS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #17: **UNIGRAMS + TRIGRAMS** & Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    rude = True              neg : pos    =     32.1 : 1.0\n",
      "                    half = True              neg : pos    =     15.5 : 1.0\n",
      "                    okay = True              neg : pos    =     11.7 : 1.0\n",
      "                   whole = True              neg : pos    =     10.7 : 1.0\n",
      "                  making = True              neg : pos    =     10.7 : 1.0\n",
      "                 request = True              neg : pos    =     10.7 : 1.0\n",
      "                occasion = True              neg : pos    =     10.7 : 1.0\n",
      "                    fact = True              neg : pos    =     10.7 : 1.0\n",
      "                    told = True              neg : pos    =     10.7 : 1.0\n",
      "                    care = True              neg : pos    =     10.7 : 1.0\n",
      "0.7266666666666667\n",
      "pos precision: 0.987012987012987\n",
      "pos recall: 0.6551724137931034\n",
      "pos F-measure: 0.7875647668393781\n",
      "neg precision: 0.4520547945205479\n",
      "neg recall: 0.9705882352941176\n",
      "neg F-measure: 0.616822429906542\n"
     ]
    }
   ],
   "source": [
    "rl = zip(reviews,ratings)\n",
    "\n",
    "#define a bag_of_words function to return word, True. Only I am renaming it to the hotel's name for simplicity\n",
    "\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile='english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "# Import Bigram finder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "\n",
    "# Import Bigram metrics - we will use these to identify the top 200 bigrams\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "def bag_of_trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq, n=100):\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(words)\n",
    "    trigrams = trigram_finder.nbest(score_fn, n)\n",
    "    return bag_of_words(trigrams + words)\n",
    "    \n",
    "    trigrams = bag_of_trigrams_words(words)\n",
    "\n",
    "#Creating our featureset dictionary for modeling\n",
    "\n",
    "unitri_fset = []\n",
    "\n",
    "for k, v in rl:\n",
    "    bag_of_trigrams_words(k)\n",
    "    unitri_fset.append((bag_of_trigrams_words(k),v))\n",
    "\n",
    "import random\n",
    "random.shuffle(unitri_fset)\n",
    "\n",
    "#splits the data around 70% of 500 *350 reviews* for both testing and training\n",
    "\n",
    "train_set, test_set = unitri_fset[0:350], unitri_fset[350:]\n",
    "\n",
    "#Now we will calculate accuracy, precision, recall, and f-measure using Naives Bayes classifier\n",
    "#This will also show the top 10 most informative features of our featureset\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nb_classifier.show_most_informative_features(10)\n",
    "\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(nb_classifier, test_set))\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "    \n",
    "for i, (unitri_fset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(unitri_fset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we do not see the trigrams forming in most informative features, we do however see a big improvement in **negative recall** already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #18 **UNIGRAMS + TRIGRAMS** Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7933333333333333\n",
      "pos precision: 0.7891156462585034\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8821292775665399\n",
      "neg precision: 0.4520547945205479\n",
      "neg recall: 0.9705882352941176\n",
      "neg F-measure: 0.616822429906542\n"
     ]
    }
   ],
   "source": [
    "#Making a decision tree model to compare which is the better performing model\n",
    "import collections\n",
    "from nltk import metrics\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(dt_classifier, test_set))\n",
    "    \n",
    "for i, (unitri_fset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = dt_classifier.classify(unitri_fset)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #19 **UNIGRAMS + TRIGRAMS** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.7733333333333333\n",
      "pos recall: 1.0\n",
      "pos F-measure: 0.8721804511278195\n",
      "neg precision: 0.4520547945205479\n",
      "neg recall: 0.9705882352941176\n",
      "neg F-measure: 0.616822429906542\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    " \n",
    "for i, (unitri_fset, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = logit_classifier.classify(unitri_fset)\n",
    "    testsets[observed].add(i)\n",
    "  \n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, SVM appeared to not be working with unigrams and trigrams (or probably just needs to be further adjusted in the future as I continue to improve my Python skills). We can, however, still analyze which model has the best negative recall. While modeling a combination of unigrams, bigrams, and trigrams would probably be the most informative, it appears that decision tree model using both unigrams and trigrams has the highest negative recall of 0.976. Even though the trigrams do not appear on the most informative features, both trigrams and unigrams make a more contextual dataset to model with. It was more entertaining though to see the Unigrams and Bigrams informative features, as both appeared in informative features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
